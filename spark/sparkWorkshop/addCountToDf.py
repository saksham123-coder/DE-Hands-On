#assuming i have a df
from pyspark.sql.window import Window
from pyspark.sql.functions import *
data = [("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2)]
columns = ["column0", "column1", "column2", "label"]
df = spark.createDataFrame(data, schema=columns)

window_spec = Window.partitionBy("column1","column2")

df = df.withColumn("count",count("label").over(window_spec))
